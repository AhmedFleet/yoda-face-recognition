import cv2
import numpy as np
from PIL import Image
from sentence_transformers import SentenceTransformer
import psycopg2
import os
import matplotlib.pyplot as plt

# ========== ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ CLIP ==========
model = SentenceTransformer("clip-ViT-B-32")

# ========== ØªØ­Ù…ÙŠÙ„ ÙƒØ§Ø´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡ ==========
haar_cascade = cv2.CascadeClassifier("haarcascade_frontalface_default.xml")

# ========== Ù…Ø³Ø§Ø± ØµÙˆØ±Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ==========
query_image_path = "testFaces/alan.png"  # â† Ø¹Ø¯Ù‘Ù„ Ø­Ø³Ø¨ Ø§Ø³Ù… ØµÙˆØ±ØªÙƒ
img = cv2.imread(query_image_path)

if img is None:
    print(f"[âŒ] Ø§Ù„ØµÙˆØ±Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©: {query_image_path}")
    exit()

# ========== Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ÙˆØ¬Ù‡ ==========
gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
faces = haar_cascade.detectMultiScale(gray_img, scaleFactor=1.001, minNeighbors=380, minSize=(100, 100))

if len(faces) == 0:
    print("[âš ï¸] Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙˆØ¬Ù‡ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©.")
    exit()

(x, y, w, h) = faces[0]
cropped_face = img[y:y + h, x:x + w]

try:
    face_pil = Image.fromarray(cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB)).convert("RGB")
    face_pil = face_pil.resize((224, 224))
except Exception as e:
    print(f"[âŒ] ÙØ´Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙˆØ¬Ù‡: {e}")
    exit()

# ========== Ø­Ø³Ø§Ø¨ embedding ==========
try:
    embedding = model.encode(face_pil)
    print(f"[âœ…] Ø­Ø¬Ù… embedding: {len(embedding)}")
except Exception as e:
    print(f"[âŒ] ÙØ´Ù„ ÙÙŠ Ø­Ø³Ø§Ø¨ embedding: {e}")
    exit()

# ========== ØªØ­ÙˆÙŠÙ„ embedding Ø¥Ù„Ù‰ ØµÙŠØºØ© PostgreSQL vector ==========
vector_str = f"[{', '.join(map(str, embedding))}]"

# ========== Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ==========
try:
    conn = psycopg2.connect(
        host="localhost",
        dbname="postgres",
        user="postgres",
        password="123",
        port=5432
    )
    cur = conn.cursor()
except Exception as e:
    print(f"[âŒ] Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
    exit()

# ========== Ø§Ø³ØªØ¹Ù„Ø§Ù… PostgreSQL Ù„Ø§ÙŠØ¬Ø§Ø¯ Ø£Ù‚Ø±Ø¨ ÙˆØ¬Ù‡ ==========
try:
    cur.execute("""
        SELECT picture, 1 - (embedding <=> %s::vector) AS similarity
        FROM pictures
        ORDER BY embedding <=> %s::vector
        LIMIT 1
    """, (vector_str, vector_str))
    result = cur.fetchone()
except Exception as e:
    print(f"[âŒ] Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {e}")
    conn.close()
    exit()

# ========== Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø© ==========
if result:
    name, similarity = result
    percentage = similarity * 100
    print(f"[ğŸ‘¤] closeest face: {name}")
    print(f"[ğŸ“Š]: {percentage:.2f}%")
else:
    print("[âŒ] Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬.")
    conn.close()
    exit()

conn.close()

# ========== Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±ØªÙŠÙ† Ø¬Ù†Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ ==========
query_image = Image.open(query_image_path).convert("RGB")
closest_image_path = os.path.join("stored-faces", name)
closest_image = Image.open(closest_image_path).convert("RGB")

fig, axes = plt.subplots(1, 2, figsize=(10, 5))
axes[0].imshow(query_image)
axes[0].set_title("ØµÙˆØ±Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…")
axes[0].axis("off")

axes[1].imshow(closest_image)
axes[1].set_title(f"Ø£Ù‚Ø±Ø¨ ÙˆØ¬Ù‡: {name}\n({percentage:.2f}%)")
axes[1].axis("off")

plt.tight_layout()
plt.show()
