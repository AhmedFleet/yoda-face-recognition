import streamlit as st
import cv2
from PIL import Image
import numpy as np
from sentence_transformers import SentenceTransformer
import psycopg2
import tempfile
import os

# ========== ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ CLIP ==========
model = SentenceTransformer("clip-ViT-B-32")

# ========== ØªØ­Ù…ÙŠÙ„ ÙƒØ§Ø´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡ ==========
haar_cascade = cv2.CascadeClassifier("haarcascade_frontalface_default.xml")

# ========== ÙˆØ§Ø¬Ù‡Ø© Streamlit ==========
st.title("ğŸ” Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¬ÙˆÙ‡")

uploaded_file = st.file_uploader("Ø§Ø±ÙØ¹ ØµÙˆØ±Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙˆØ¬Ù‡ ğŸ‘¤", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    image = Image.open(uploaded_file).convert("RGB")
    st.image(image, caption="Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„ØªÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§", use_column_width=True)

    # Ø­ÙØ¸ Ù…Ø¤Ù‚Øª Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ù€ OpenCV
    with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp:
        image.save(tmp.name)
        img_cv = cv2.imread(tmp.name)

    gray_img = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)
    faces = haar_cascade.detectMultiScale(gray_img, scaleFactor=1.1, minNeighbors=10)

    if len(faces) == 0:
        st.warning("âŒ Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø£ÙŠ ÙˆØ¬Ù‡.")
    else:
        x, y, w, h = faces[0]
        face = img_cv[y:y+h, x:x+w]
        face_pil = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB)).resize((224, 224))

        try:
            embedding = model.encode(face_pil)
            vector_str = f"[{', '.join(map(str, embedding))}]"
        except Exception as e:
            st.error(f"[âŒ] ÙØ´Ù„ ÙÙŠ Ø­Ø³Ø§Ø¨ embedding: {e}")
            st.stop()

        # Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª PostgreSQL
        try:
            conn = psycopg2.connect(
                host="localhost",
                dbname="postgres",
                user="postgres",
                password="123",
                port=5432
            )
            cur = conn.cursor()
        except Exception as e:
            st.error(f"[âŒ] Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            st.stop()

        try:
            cur.execute("""
                SELECT picture, 1 - (embedding <=> %s::vector) AS similarity
                FROM pictures
                ORDER BY embedding <=> %s::vector
                LIMIT 1
            """, (vector_str, vector_str))
            result = cur.fetchone()
            conn.close()

            if result:
                name, similarity = result
                percent = similarity * 100
                st.success(f"ğŸ‘¤ Ø£Ù‚Ø±Ø¨ ÙˆØ¬Ù‡: {name}")
                st.metric("Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡", f"{percent:.2f}%")
                stored_face_path = os.path.join("stored-faces", name)
                if os.path.exists(stored_face_path):
                    st.image(stored_face_path, caption="Ø£Ù‚Ø±Ø¨ ÙˆØ¬Ù‡ Ù…Ø·Ø§Ø¨Ù‚", use_column_width=True)
                else:
                    st.warning("ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªÙŠØ¬Ø©ØŒ Ù„ÙƒÙ† Ø§Ù„ØµÙˆØ±Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ†.")
            else:
                st.warning("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙˆØ¬Ù‡ Ù…Ø´Ø§Ø¨Ù‡ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        except Exception as e:
            st.error(f"[âŒ] Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {e}")
