import cv2
from PIL import Image
from torchvision import transforms
from sentence_transformers import SentenceTransformer
import psycopg2
import os
import shutil

# =============== ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ CLIP ===============
model = SentenceTransformer("clip-ViT-B-32")
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor()
])

# =============== ØªØ­Ù…ÙŠÙ„ ÙƒØ§Ø´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡ ===============
haar_cascade = cv2.CascadeClassifier("haarcascade_frontalface_default.xml")

# =============== ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© ===============
file_name = "two-and-a-half-men-cast-stars-characters-malibu-beach-gallery-chuck-lorre-productions-the-tannenbaum-company-warner-bros-television-cbs_1-2241006998.jpg"
img = cv2.imread(file_name)
if img is None:
    print(f"[âŒ] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©: {file_name}")
    exit()

gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# =============== ÙƒØ´Ù Ø§Ù„ÙˆØ¬ÙˆÙ‡ ===============
faces = haar_cascade.detectMultiScale(
    gray_img,
    scaleFactor=1.1,
    minNeighbors=20,
    minSize=(50, 50)
)
print(f"[â„¹ï¸] Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ¬ÙˆÙ‡ Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {len(faces)}")

# =============== Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù‚Ø¯ÙŠÙ… ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯ÙŠØ¯ ===============
if os.path.exists("stored-faces"):
    shutil.rmtree("stored-faces")
os.makedirs("stored-faces", exist_ok=True)
# =============== Ù‚Øµ Ø§Ù„ÙˆØ¬ÙˆÙ‡ ÙˆØ­ÙØ¸Ù‡Ø§ ===============
for i, (x, y, w, h) in enumerate(faces):
    cropped_face = img[y:y + h, x:x + w]
    face_path = f"stored-faces/{i}.jpg"
    cv2.imwrite(face_path, cropped_face)
    print(f"[âœ”] ØªÙ… Ø­ÙØ¸ Ø§Ù„ÙˆØ¬Ù‡: {face_path}")

# =============== Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ===============
try:
    conn = psycopg2.connect(
        host="localhost",
        dbname="postgres",
        user="postgres",
        password="123",
        port=5432
    )
    cur = conn.cursor()
except Exception as e:
    print(f"[âŒ] ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
    exit()

# =============== Ø­Ø°Ù Ø§Ù„ØµÙˆØ± Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ===============
try:
    cur.execute("DELETE FROM pictures")
    conn.commit()
    print("[ğŸ—‘] ØªÙ… Ø­Ø°Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
except Exception as e:
    print(f"[âŒ] ÙØ´Ù„ Ø­Ø°Ù Ø§Ù„ØµÙˆØ± Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
    conn.close()
    exit()

# =============== Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙˆØ­Ø³Ø§Ø¨ embeddings ===============
for picture in os.listdir("stored-faces"):
    image_path = os.path.join("stored-faces", picture)

    # Ø­Ø°Ù Ø§Ù„ØµÙˆØ± Ø§Ù„ÙØ§Ø±ØºØ©
    if os.path.getsize(image_path) == 0:
        print(f"[ğŸ—‘] Ø­Ø°Ù ØµÙˆØ±Ø© ÙØ§Ø±ØºØ©: {picture}")
        os.remove(image_path)
        continue

    # ÙØªØ­ Ø§Ù„ØµÙˆØ±Ø©
    try:
        img_pil = Image.open(image_path).convert("RGB").resize((224, 224))
    except Exception as e:
        print(f"[âŒ] Ù„Ù… ÙŠÙ…ÙƒÙ† ÙØªØ­ Ø§Ù„ØµÙˆØ±Ø© {picture}: {e}")
        continue

    # Ø­Ø³Ø§Ø¨ embedding
    try:
        embedding = model.encode(img_pil)
        print(f"[âœ…] {picture} - Ø­Ø¬Ù… embedding: {len(embedding)}")
    except Exception as e:
        print(f"[âš ï¸] ÙØ´Ù„ ÙÙŠ Ø­Ø³Ø§Ø¨ embedding Ù„Ù€ {picture}: {e}")
        continue

    # Ø¥Ø¯Ø®Ø§Ù„ Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    try:
        cur.execute("""
            INSERT INTO pictures (picture, embedding)
            VALUES (%s, %s)
            ON CONFLICT (picture) DO NOTHING
        """, (picture, embedding.tolist()))
        print(f"[â†‘] ØªÙ… ØªØ®Ø²ÙŠÙ†: {picture}")
    except Exception as e:
        print(f"[âŒ] Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ {picture}: {e}")
        continue

# =============== Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø§ØªØµØ§Ù„ ===============
conn.commit()
conn.close()
print("[âœ…] ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ø¨Ù†Ø¬Ø§Ø­.")
